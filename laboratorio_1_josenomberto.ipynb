{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/josenomberto/UTEC-CDIAV3-MCD8009/blob/main/laboratorio_1_josenomberto.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "149c716d-2979-4493-96f0-3be68f821afa",
      "metadata": {
        "id": "149c716d-2979-4493-96f0-3be68f821afa"
      },
      "source": [
        "# MCD8009: Data Discovery - Laboratorio 1"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "004c2f66",
      "metadata": {
        "id": "004c2f66"
      },
      "source": [
        "**Integrantes**\n",
        "\n",
        "| N° | Código | Nombres |  Contribución (0% - 100%) |\n",
        "|----|--------|---------|---------------------------|\n",
        "| 1  |        | José Carlos Nomberto        | 100%                           |\n",
        "| 2  |        |         |                           |\n",
        "| 3  |        |         |                           |\n",
        "| 4  |        |         |                           |"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5de1bf7c",
      "metadata": {
        "id": "5de1bf7c"
      },
      "source": [
        "### Indicaciones\n",
        "\n",
        "- El laboratorio podrá resolverse de manera **individual o en equipos de hasta cuatro (4) personas**. Deberán completar los datos de todos los integrantes, y **una sola persona realizará la entrega del archivo ipynb**.\n",
        "\n",
        "- Salvo que se indique explícitamente lo contrario, no se prohibe el uso de herramientas de Inteligencia Artificial Generativa, siempre que los integrantes comprendan y puedan explicar el proceso y los resultados obtenidos. **Las respuestas no deben consistir en transcripciones literales de resultados generados por estas herramientas, sino evidenciar comprensión del tema por parte del estudiante o del equipo.**\n",
        "\n",
        "- En caso de utilizar herramientas de IA Generativa, cada equipo es responsable de verificar la coherencia de las respuestas presentadas. Si se detectan errores, inconsistencias o falta de comprensión, la pregunta podrá ser anulada sin derecho a reclamo.\n",
        "\n",
        "- En todos los casos, deberá completarse la **Declaración de Uso de IA Generativa.**\n",
        "\n",
        "- Pueden agregar libremente celdas de código o de Markdown según lo consideren conveniente."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "e7c12852",
      "metadata": {
        "id": "e7c12852"
      },
      "source": [
        "### Declaración de uso de IA Generativa\n",
        "- Indicar de manera breve la(s) herramienta(s) y/o modelo(s) de IA Generativa utilizados, especificando en qué pregunta(s) se emplearon y con qué propósito.\n",
        "- En caso no se haya usado, también indicarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "dd6cd0af",
      "metadata": {
        "id": "dd6cd0af"
      },
      "source": [
        "Se utilizó Google Gemini para consultas especificas en partes donde no encontré la solución, como por ejemplo:\n",
        "\n",
        "*   En la pregunta 4, logré completar el requerimiento leyendo el CSS del HTML, pero solamente obtenía las primeras 25 películas y no el total de 250. Se encontró que leyendo el \"script\" con id \"__NEXT_DATA__\", se obtenía la información de todas las 250 peliculas en formato JSON.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "739d301c",
      "metadata": {
        "id": "739d301c"
      },
      "source": [
        "## INICIO DEL LABORATORIO"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3b7e23a7",
      "metadata": {
        "id": "3b7e23a7"
      },
      "source": [
        "### Parte 1: Frameworks de la industria: CRISP-DM [Sin uso de IA generativa] (3 puntos)\n",
        "\n",
        "CRISP-DM suele presentarse como un framework de Ciencia de Datos, pero muchas de sus fases se aplican también en proyectos de analytics, BI y toma de decisiones data-driven, incluso sin usar modelos de machine learning. ¿En qué medida reconoce que, consciente o inconscientemente, ha trabajado siguiendo las fases de CRISP-DM en su experiencia profesional?\n",
        "Conversen entre los integrantes del equipo y presenten solo 1 caso.\n",
        "\n",
        "\n",
        "Contraste su forma real de trabajo con las fases del framework:\n",
        "\n",
        "- Business Understanding\n",
        "\n",
        "- Data Understanding\n",
        "\n",
        "- Data Preparation\n",
        "\n",
        "- Modeling (si aplica)\n",
        "\n",
        "- Evaluation\n",
        "\n",
        "- Deployment / uso en negocio"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d8ce3010",
      "metadata": {
        "id": "d8ce3010"
      },
      "source": [
        "En mi rol como Consultor de Redes de Datos y Telecomunicaciones, no he trabajado directamente con el framework CRISP-DM, sin embargo la metodología de trabajo para los proyectos de Diseño, Implementación y Operación de Redes en los cuales he participado pueden relacionarse con las fases del CRISP-DM.\n",
        "\n",
        "Por ejemplo, para proyectos \"greenfield\" de Implementación de Red MPLS de un proveedor de telecomunicaciones regional, se siguen las siguientes fases:\n",
        "\n",
        "\n",
        "**1.   Business Understanding:**\n",
        "Se definen los requerimientos del cliente para el proyecto orientado a cumplir los objetivos del negocio: diseño por capas, redundancia de equipos, redundancia de enlaces, baja latencia y jitter, politicas de QoS, etc.\n",
        "\n",
        "\n",
        "**2.   Data Understanding:**\n",
        "Se recopila informacion de los equipos y la red, como archivos de configuración, diagramas de topología, tablas de enrutamiento, logs de tráfico, y se ejecutan evaluaciones de la red, para entender el estado actual.\n",
        "\n",
        "\n",
        "**3.   Data Preparation:**\n",
        "Se selecciona el direccionamiento IP, los protocolos de enrutamiento, los servicios de red y se generan los documentos de diseño HLD (High Level Design) y LLD (Low Level Design).\n",
        "\n",
        "\n",
        "**4.   Modeling:**\n",
        "Se generan los documento de Implementacion NIP (Network Implementation Plan), para configurar los equipos con el diseño propuesto.\n",
        "\n",
        "\n",
        "**5.   Evaluation:**\n",
        "Se ejecutan las pruebas de validacion (Network Ready for Use) de  disponibilidad y performance de la red. En caso no se cumplan algunas pruebas, se revisa el diseño para tomar las correcciones necesarias.\n",
        "\n",
        "\n",
        "**6.   Deployment:**\n",
        "Paso a producción migrando los servicios de la red actual a la nueva red en ventanas de migracion planificadas para minimizar el impacto. Se inicia el monitoreo continuo para asegurar la operación de la red.\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "5b5c7cfe",
      "metadata": {
        "id": "5b5c7cfe"
      },
      "source": [
        "### Parte 2: Tipos de analítica [Sin uso de IA generativa] (3 puntos)\n",
        "\n",
        "**Caso: Análisis de datos para una tienda en línea**\n",
        "\n",
        "Imagine que es consultor de análisis de datos y has sido contratado por una tienda en línea que vende productos electrónicos. La tienda desea aprovechar al máximo los datos recopilados de sus clientes y mejorar su estrategia de marketing, la experiencia del cliente y la toma de decisiones comerciales. Tu objetivo es aplicar diferentes tipos de análisis de datos para obtener información valiosa y proporcionar recomendaciones basadas en los resultados obtenidos.\n",
        "\n",
        "La tienda en línea ha recopilado una amplia variedad de datos sobre sus clientes y las transacciones realizadas. Entro los datos demográficos con los que cuenta tenemos: edad, género, ubicación geográfica, etc. Asimismo, cuenta con datos de navegación por el sitio web en donde se puede ver los productos vistos, los comentarios de los clientes, entre otros.\n",
        "\n",
        "Tu misión es utilizar diferentes enfoques de análisis de datos para ayudar a la tienda a comprender mejor a sus clientes, optimizar sus operaciones y tomar decisiones informadas. En función de la información dada y supuestos que pueda añadir, realice los 4 Tipos de Data Analytics estudiados comentando qué deberíamos hacer en cada uno de ellos."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "cff1bbea",
      "metadata": {
        "id": "cff1bbea"
      },
      "source": [
        "**Analítica Descriptiva**\n",
        "\n",
        "Para entender el estado actual:\n",
        "\n",
        "*   Generar KPIs de Ventas: volumen total de ventas, productos con más vistas, productos más vendidos, etc.\n",
        "*   Segmentar a los clientes: por género, por rango de edad, por ubicación, medio de pago, etc.\n",
        "*   Analizar experiencia de usuario: identificar las causas de abandono o cancelacion de compra, tiempo de respuesta en la pagina web, respuesta de la plataforma de pagos.\n",
        "\n",
        "Supuesto para el análisis descriptivo:\n",
        "\n",
        "*   Los datos muestran que la ubicacion con menor porcentaje de ventas es Huancayo\n",
        "El producto con más vistas es una laptop de gama media\n",
        "\n",
        "\n",
        "**Analítica Diagnóstico**\n",
        "\n",
        "Para idenficar problemas:\n",
        "\n",
        "*   Análisis de Correlación: Determinar si existe una relación entre el tiempo de navegación en las reseñas de productos y la decisión de compra.\n",
        "*   Entender las causas de devoluciones de productos procesando los comentarios de los clientes (tiempo de entrega, producto dañado, producto no como lo esperado, etc).\n",
        "*   Investigar si hubo cambios en precios de la competencia, campañas de oferta relampago, fallos en plataforma de pagos, que hayan generado baja de ventas en segmento o producto\n",
        "\n",
        "Supuesto del análisis de diagnostico:\n",
        "*   Se descubre que hay disminucion en las ventas en Huancayo porque hay quejas en tiempo alto de entrega.\n",
        "\n",
        "\n",
        "**Analítica Predictiva**\n",
        "\n",
        "Para predecir variables, se usan modelos de ML basados en datos historicos:\n",
        "\n",
        "*   Modelos para predicción de compra: predecir cuantos clientes tienen mayor probabilidad de comprar un producto adicional después de haber comprado uno antes (upselling).\n",
        "*   Modelos para predicción de fuga: identificar clientes habituales muestran vaya a dejar de comprar (comentarios negativos)\n",
        "*   Modelos para pronóstico de demanda: estimar el inventario requerido para venta adecuada de productos en campaña.\n",
        "\n",
        "Supuesto al análisis predictivo:\n",
        "*   El modelo de prediccion detecta que las ventas de laptops de gama media se incrementara durante la proxima campaña escolar.\n",
        "\n",
        "\n",
        "**Analítica Prescriptiva**\n",
        "\n",
        "Recomendar acciones especificas:\n",
        "\n",
        "*   Recomendaciones personalizadas por usuario\n",
        "*   Ajustar los precios de productos basado en la demanda y stock\n",
        "*   Automatizacion de envio de ofertas a clientes en riesgo de abandono\n",
        "\n",
        "Supuesto al análisis prescriptivo:\n",
        "*   La tienda detecta si un usuario en Huancayo visualiza una laptop de gama media, para ofrecer el envio gratis asegurando un mejor tiempo de entrega con el uso de un mejor proveedor\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "d9fcc9fa",
      "metadata": {
        "id": "d9fcc9fa"
      },
      "source": [
        "### Parte 3: Lectura de datos (5 puntos)\n",
        "\n",
        "En clase vimos cómo leer datos desde distintas fuentes. En la industria es muy común recibir archivos planos como CSV; sin embargo, con frecuencia no se conoce de antemano:\n",
        "\n",
        "- El *encoding* del archivo.\n",
        "- El delimitador utilizado (`;`, `,`, `|`, `\\t`, etc.).\n",
        "- La existencia de líneas \"basura\" al inicio (metadatos, comentarios, encabezados duplicados, etc.).\n",
        "\n",
        "El objetivo es leer correctamente los datos del archivo `lab1.csv` sin modificar el archivo original, resolviendo las incertidumbres mencionadas.\n",
        "\n",
        "Antes de programar, lea la documentación de [chardet](https://chardet.readthedocs.io/en/latest/) y de [csv.Sniffer](https://docs.python.org/es/3/library/csv.html#csv.Sniffer)\n",
        "\n",
        "Adicional a la implementación del código, imprima el head del archivo y responda las preguntas solicitadas.\n",
        "\n",
        "#### Funciones a implementar"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import chardet\n",
        "import csv\n",
        "import pandas as pd\n",
        "\n",
        "\n",
        "def detectar_encoding(nombre_archivo, longitud_muestra):\n",
        "    \"\"\"\n",
        "    Detecta la codificación del archivo.\n",
        "    Argumentos:\n",
        "      nombre_archivo: nombre del archivo a leer\n",
        "      longitud_muestra: número de bytes a leer para la muestra\n",
        "    Retorna:\n",
        "      codificacion: diccionario con la codificación detectada\n",
        "    \"\"\"\n",
        "    # Abrir archivo y tomar una muestra de longitud_muestra para detectar la codificacion\n",
        "    with open(nombre_archivo, 'rb') as archivo:\n",
        "        codificacion = chardet.detect(archivo.read(longitud_muestra))\n",
        "\n",
        "    return codificacion\n",
        "\n",
        "\n",
        "def detectar_delimitador(nombre_archivo, longitud_muestra, codificacion, skip_lines):\n",
        "    \"\"\"\n",
        "    Detecta el delimitador del archivo CSV.\n",
        "    Argumentos:\n",
        "      nombre_archivo: nombre del archivo a leer\n",
        "      longitud_muestra: número de bytes a leer para la muestra\n",
        "      codificacion: codificación del archivo\n",
        "      skip_lines: número de líneas iniciales a ignorar\n",
        "    Retorna:\n",
        "      delimitador: delimitador del archivo\n",
        "    \"\"\"\n",
        "    # Abrir archivo con la codificacion detectada, ignorar las lineas indicadas y obtener el delimitador\n",
        "    with open(nombre_archivo, 'r', newline='', encoding=codificacion) as archivo:\n",
        "        for _ in range(skip_lines):\n",
        "            next(archivo)\n",
        "        archivo_muestra = archivo.read(longitud_muestra)\n",
        "        archivo.seek(0)\n",
        "        delimitador = csv.Sniffer().sniff(archivo_muestra, delimiters=[',', ';', '|', '\\t']).delimiter\n",
        "\n",
        "    return delimitador\n",
        "\n",
        "\n",
        "def leer_csv_inteligente(ruta_archivo, skip_lines=0):\n",
        "    \"\"\"\n",
        "    Lee un archivo CSV detectando automáticamente:\n",
        "    - El encoding\n",
        "    - El delimitador\n",
        "\n",
        "    Argumentos:\n",
        "    - ruta_archivo: ruta al archivo CSV\n",
        "    - skip_lines: número de líneas iniciales a ignorar\n",
        "\n",
        "    Retorna:\n",
        "    - pandas.DataFrame\n",
        "    \"\"\"\n",
        "    longitud_muestra = 600\n",
        "\n",
        "    # 1. Detectando la codificación del archivo\n",
        "    print(\"\\n\")\n",
        "    print(f\"Detectando la codificación del archivo ... \")\n",
        "    encoding = detectar_encoding(ruta_archivo, longitud_muestra)\n",
        "    print(f\"   La codificación del archivo '{ruta_archivo}' es:  {encoding['encoding']}\")\n",
        "    print(f\"   El nivel de confianza para la codificación detectada es:  {encoding['confidence']}\")\n",
        "\n",
        "    # 2. Detectando el delimitador del archivo\n",
        "    print(\"\\n\")\n",
        "    print(f\"Detectando el delimitador del archivo ... \")\n",
        "    delimitador = detectar_delimitador(ruta_archivo, longitud_muestra, encoding['encoding'], skip_lines)\n",
        "    print(f\"   El delimitador del archivo '{ruta_archivo}' es:  '{delimitador}'\")\n",
        "\n",
        "    # 3. Abriendo el archivo con codificacion, delimitador y saltando lineas\n",
        "    try:\n",
        "        # Leer archivo en DataFrame\n",
        "        print(\"\\n\")\n",
        "        print(f\"Leyendo el archivo ... \")\n",
        "        df = pd.read_csv(\n",
        "            ruta_archivo,\n",
        "            sep=delimitador,\n",
        "            #sep='|',\n",
        "            encoding=encoding['encoding'],\n",
        "            skiprows=skip_lines\n",
        "        )\n",
        "        # Imprime la cabecera del DataFrame\n",
        "        print(f\"Imprimiendo cabecera del archivo ... \")\n",
        "        print(\"\\n\")\n",
        "        print(df.head())\n",
        "\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: Archivo '{ruta_archivo}' no encontrado.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error: {e}\")\n",
        "\n",
        "    return df\n"
      ],
      "metadata": {
        "collapsed": true,
        "id": "Tr11IhdC_9po"
      },
      "id": "Tr11IhdC_9po",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Lectura inteligente de archivo csv\n",
        "nombre_archivo = \"lab1.csv\"\n",
        "skip_lines = 1\n",
        "print(f\"Lectura interligente de archivo '{nombre_archivo}' con '{skip_lines}' línea(s) inicial(es) a ignorar:\")\n",
        "archivo_df = leer_csv_inteligente(nombre_archivo, skip_lines)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EtoL609MHqgQ",
        "outputId": "eb33a14d-83f5-4f91-a3e4-23fab899614a"
      },
      "id": "EtoL609MHqgQ",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Lectura interligente de archivo 'lab1.csv' con '1' línea(s) inicial(es) a ignorar:\n",
            "\n",
            "\n",
            "Detectando la codificación del archivo ... \n",
            "   La codificación del archivo 'lab1.csv' es:  ISO-8859-1\n",
            "   El nivel de confianza para la codificación detectada es:  0.73\n",
            "\n",
            "\n",
            "Detectando el delimitador del archivo ... \n",
            "   El delimitador del archivo 'lab1.csv' es:  '|'\n",
            "\n",
            "\n",
            "Leyendo el archivo ... \n",
            "Imprimiendo cabecera del archivo ... \n",
            "\n",
            "\n",
            "   id nombre apellido  edad    monto    estado\n",
            "0   1   José   García    20  4210.24    ACTIVO\n",
            "1   2  María    Pérez    44  3950.89  INACTIVO\n",
            "2   3   Peña    López    56  2890.01    ACTIVO\n",
            "3   4   Luis  Sánchez    50  1962.01    ACTIVO\n",
            "4   5    Ana   Torres    51  4075.63  INACTIVO\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0881a668",
      "metadata": {
        "id": "0881a668"
      },
      "source": [
        "\n",
        "#### Requisitos técnicos\n",
        "\n",
        "La solución debe cumplir con lo siguiente:\n",
        "\n",
        "- Detectar el encoding del archivo utilizando `chardet`.\n",
        "- Detectar el delimitador del archivo CSV utilizando `csv.Sniffer`.\n",
        "- Incluir el argumento `skip_lines` como el número de líneas iniciales a ignorar.\n",
        "- Devolver los datos en una estructura `pandas.DataFrame`.\n",
        "- Optimizar la detección del encoding y delimitador (tomar un sample del archivo, no un full read)\n",
        "\n",
        "\n",
        "#### Preguntas teóricas\n",
        "\n",
        "Responda brevemente las siguientes preguntas:\n",
        "\n",
        "1. ¿Qué problema resuelve **chardet**?  \n",
        "   ¿Qué información devuelve el método `chardet.detect()`?\n",
        "\n",
        "2. ¿Para qué sirve **csv.Sniffer**?  \n",
        "   ¿Cuáles son sus principales limitaciones?  \n",
        "   ¿Qué sucede si `csv.Sniffer` detecta incorrectamente el delimitador?\n",
        "\n",
        "3. ¿Cuál era el encoding y el delimitador del archivo `lab1.csv`?\n",
        "\n",
        "4. ¿Por qué no siempre es una buena práctica confiar únicamente en la detección automática?\n",
        "\n",
        "5. En la función propuesta, el argumento `skip_lines` se determina de manera manual.  \n",
        "   ¿Qué alternativas podrían implementarse para automatizar la detección de las líneas a ignorar?\n",
        "\n",
        "6. Desde un criterio de negocio o de mejora de procesos, ¿qué mejora general propondría para gestionar de forma más eficiente la recepción de este tipo de archivos “problemáticos”?"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**RESPUESTAS:**\n",
        "\n",
        "1.   La libreria chardet sirve para detectar la codificacion de un archivo.\n",
        "Devuelve un diccionario con la información de la codificacion detectada y el nivel de confianza para la detección.\n",
        "\n",
        "2.   La libreria csv.Sniffer sirve para detectar el formato de un archivo CSV analizando una muestra del texto. Las principales limitaciones son:\n",
        "  *   no detecta los caracteres de escape de secuencia\n",
        "  *   resultados inconsistentes segun el orden de los datos\n",
        "\n",
        "      Si csv.Sniffer detecta incorrectamente el delimitador, la importacion del archivo fallara causando que los datos no esten bien estructurados o se carguen todos en una misma columna\n",
        "\n",
        "3.   El encoding del archivo es 'ISO-8859-1', y el delimtador '|'.\n",
        "\n",
        "4.   No es bueno solo confiar en la detección automática ya que es suceptible a errores de interferencia para detectar el tipo de datos (por ejemplo, fechas), detección incorrecta en archivos sin cabecera, el sniffer analiza solamente hasta los primero 2048 bytes lo cual podria no ser suficiente, etc.\n",
        "\n",
        "5.   Alternativas al uso del parametro skip_lines definido manualmente:\n",
        "\n",
        "*   Leer el archivo linea por linea\n",
        "*   Identificar lineas con espacios en blanco\n",
        "*   Ignorar lineas en blanco\n",
        "*   Ignorar lineas comentadas (#)\n",
        "\n",
        "6.   Mejoras en el proceso:\n",
        "\n",
        "*   Definir estructura (formato) para archivos a leer\n",
        "*   Estandarizar el encoding a uno estandar para todos los archivos\n",
        "*   Definir cabecera de archivos para uniformizar y adaptar aquellos que no tengan cabecera\n",
        "*   Mostrar métricas en la lectura de archivos, como por ejemplo archivos \"limpios\" vs archivos \"sucios\",\n",
        "\n"
      ],
      "metadata": {
        "id": "DBrVFDBSH1qY"
      },
      "id": "DBrVFDBSH1qY"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c2cf9382",
      "metadata": {
        "id": "c2cf9382"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "id": "86727bef-7dad-4731-bf45-98733e3b9f2b",
      "metadata": {
        "id": "86727bef-7dad-4731-bf45-98733e3b9f2b"
      },
      "source": [
        "### Parte 4: Webscrapping (5 puntos)"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3ec062ad-fbad-46a0-bf67-2c84af84723e",
      "metadata": {
        "id": "3ec062ad-fbad-46a0-bf67-2c84af84723e"
      },
      "source": [
        "En clase vimos cómo aplicar webscraping. Ahora es momento de practicar. Puede usar la(s) librería(s) que considere más conveniente para esta casuística.\n",
        "\n",
        "La actividad consiste en obtener un dataframe de esta página: https://www.imdb.com/chart/top/, la cual muestra las 250 mejores películas de IMDb.\n",
        "\n",
        "Los campos a extraer son los siguientes, con el siguiente formato (se muestra un registro a modo de ejemplo):\n",
        "\n",
        "| rankings | titulo           | anio  | duracion_minutos   | clasificacion | calificacion |  votos_millones       |\n",
        "|----------|------------------|-------|--------------------|---------------|--------------|-----------------------|\n",
        "| 1        | Sueño de fuga    | 1994  |    142             |       B       |    9.3       |  2.2                  |\n",
        "\n",
        "- Note que la duración se ha convertido a minutos y que los votos han sido transformados a millones (como variable numérica).\n",
        "- Aplique buenas prácticas de webscrapping (ej: Uso de User-Agent en los headers HTTP, uso de timeout en la petición, manejo de errores HTTP, etc.)\n",
        "- Sustente brevemente las razones por las que empleó dicha(s) librería(s) de webscrapping.\n",
        "- Finalmente, imprimir el head del dataframe, además, exportar dicho dataframe en un archivo llamado `top250_imdb.csv`. No adjunte el archivo en su entrega, pero debe realizar el código para generarlo."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SOLUCION***"
      ],
      "metadata": {
        "id": "AMY3c20m0oSN"
      },
      "id": "AMY3c20m0oSN"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "206b8408",
      "metadata": {
        "id": "206b8408",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "918b1b52-9bd1-40f6-e995-dd5339055bb0",
        "collapsed": true
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: pandas in /usr/local/lib/python3.12/dist-packages (2.2.2)\n",
            "Requirement already satisfied: numpy>=1.26.0 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.0.2)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.12/dist-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.12/dist-packages (from pandas) (2025.3)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.12/dist-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n",
            "Collecting BeautifulSoup\n",
            "  Downloading BeautifulSoup-3.2.2.tar.gz (32 kB)\n",
            "  \u001b[1;31merror\u001b[0m: \u001b[1msubprocess-exited-with-error\u001b[0m\n",
            "  \n",
            "  \u001b[31m×\u001b[0m \u001b[32mpython setup.py egg_info\u001b[0m did not run successfully.\n",
            "  \u001b[31m│\u001b[0m exit code: \u001b[1;36m1\u001b[0m\n",
            "  \u001b[31m╰─>\u001b[0m See above for output.\n",
            "  \n",
            "  \u001b[1;35mnote\u001b[0m: This error originates from a subprocess, and is likely not a problem with pip.\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25herror\n",
            "\u001b[1;31merror\u001b[0m: \u001b[1mmetadata-generation-failed\u001b[0m\n",
            "\n",
            "\u001b[31m×\u001b[0m Encountered error while generating package metadata.\n",
            "\u001b[31m╰─>\u001b[0m See above for output.\n",
            "\n",
            "\u001b[1;35mnote\u001b[0m: This is an issue with the package mentioned above, not pip.\n",
            "\u001b[1;36mhint\u001b[0m: See above for details.\n",
            "Requirement already satisfied: curl_cffi in /usr/local/lib/python3.12/dist-packages (0.14.0)\n",
            "Requirement already satisfied: cffi>=1.12.0 in /usr/local/lib/python3.12/dist-packages (from curl_cffi) (2.0.0)\n",
            "Requirement already satisfied: certifi>=2024.2.2 in /usr/local/lib/python3.12/dist-packages (from curl_cffi) (2026.1.4)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.12/dist-packages (from cffi>=1.12.0->curl_cffi) (3.0)\n"
          ]
        }
      ],
      "source": [
        "# 1. Instalar modulos requeridos\n",
        "\n",
        "# Se usa \"BeautifulSoup\" para parsear el documento HTML\n",
        "# Se usa \"curl_cffi\" ya que es más util que \"request\" para imitar las huellas dactilares de navegadores reales\n",
        "!pip install pandas\n",
        "!pip install BeautifulSoup\n",
        "!pip install curl_cffi"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2. Importar modulos requeridos\n",
        "\n",
        "from bs4 import BeautifulSoup\n",
        "from curl_cffi import requests\n",
        "import pandas as pd\n",
        "import time\n",
        "import json\n",
        "import re"
      ],
      "metadata": {
        "id": "Z2veXhhSlx7R"
      },
      "id": "Z2veXhhSlx7R",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 3. Definicion de funciones\n",
        "\n",
        "def get_imdb_top(url):\n",
        "    \"\"\"\n",
        "    Obtiene la respuesta HTTP del url\n",
        "    Argumentos:\n",
        "      url: enlace URL para web scrapping\n",
        "    Retorna:\n",
        "      response: respuesta HTTP GET\n",
        "    \"\"\"\n",
        "    # Se definen user-agent y caberas para el HTTP GET\n",
        "    user_agent = \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/121.0.0.0 Safari/537.36\"\n",
        "    headers = {\n",
        "        \"User-Agent\": user_agent,\n",
        "        \"Accept-Language\": \"en-US,en;q=0.9\",\n",
        "        \"Accept-Encoding\": \"gzip, deflate, br\",\n",
        "        'Connection': 'keep-alive',\n",
        "        \"Referer\": \"https://www.google.com/\"\n",
        "    }\n",
        "\n",
        "    print(f\"\\n1. URL para Web Scrapping: {url}\")\n",
        "    try:\n",
        "        # Para gestionar la conexion TLS se usa impersonate='chrome120'\n",
        "        response = requests.get(url, headers=headers, impersonate=\"chrome120\", timeout=20)\n",
        "        if response.status_code == 200:\n",
        "            print(f\"\\n2. Solicitud HTTP exitosa con codigo de estado de respuesta: {response.status_code}\")\n",
        "            return response\n",
        "        else:\n",
        "            print(f\"\\n2. Solicitud HTTP fallida con codigo de estado de respuesta: {response.status_code}\")\n",
        "            return None\n",
        "    except Exception as e:\n",
        "        print(f\"No se pudo obtener informacion de: {url}\")\n",
        "        print(f\"Error: {e}\")\n",
        "        return None\n",
        "\n",
        "\n",
        "def parse_imdb_json(data):\n",
        "    \"\"\"\n",
        "    Parsea la informacion de IMDB obtenida\n",
        "    Input:\n",
        "      data: datos json de NEXT_DATA__ script tag\n",
        "    Output:\n",
        "      response: dataframe con información requerida de cada pelicula\n",
        "    \"\"\"\n",
        "    try:\n",
        "        movie_edges = data['props']['pageProps']['pageData']['chartTitles']['edges']\n",
        "    except KeyError:\n",
        "        print(\"No se pudo encontrar informacion de peliculas en la estructura JSON.\")\n",
        "        return pd.DataFrame()\n",
        "\n",
        "    movies_list = []\n",
        "    for edge in movie_edges:\n",
        "        node = edge['node']\n",
        "        # Extrae informacion de peliculas y agrega a la lista de peliculas\n",
        "        movies_list.append({\n",
        "            \"rankings\": edge.get('currentRank'),\n",
        "            \"titulo\": node['titleText']['text'],\n",
        "            \"anio\": node['releaseYear']['year'],\n",
        "            \"duracion_minutos\": node['runtime']['seconds'] // 60 if node['runtime'] else 0,\n",
        "            \"clasificacion\": node['certificate']['rating'] if node.get('certificate') else \"N/A\",\n",
        "            \"calificacion\": node['ratingsSummary']['aggregateRating'],\n",
        "            \"votos_millones\": round(int(node['ratingsSummary']['voteCount']) / 1000000, 1)\n",
        "        })\n",
        "\n",
        "    # Funcion devulve la lista de informacion de peliculas como dataframe\n",
        "    return pd.DataFrame(movies_list)\n"
      ],
      "metadata": {
        "id": "nngDDYSC7cu9"
      },
      "id": "nngDDYSC7cu9",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# 4. Execute IMDB Web scrapping script\n",
        "\n",
        "# Define URL for Web Scrapping\n",
        "url = \"https://www.imdb.com/chart/top/\"\n",
        "\n",
        "try:\n",
        "    # Obtener respuesta HTTP de url\n",
        "    response = get_imdb_top(url)\n",
        "    # Si hay respuesta\n",
        "    if response:\n",
        "        # Parsea HTML y extrae informacion de peliculas\n",
        "        soup = BeautifulSoup(response.text, \"html.parser\")\n",
        "        # Localiza y extrae directamente del objeto JSON (__NEXT_DATA__)\n",
        "        script_tag = soup.find(\"script\", id=\"__NEXT_DATA__\")\n",
        "        if not script_tag:\n",
        "            print(f'\\n3. No se pudo obtener informacion JSON del script \"__NEXT_DATA__\"')\n",
        "        else:\n",
        "            # Carga script JSON\n",
        "            data = json.loads(script_tag.string)\n",
        "        # Parsea informacion IMDB a dataframe\n",
        "        df_imdb_top_250 = parse_imdb_json(data)\n",
        "        # Imprime el numero de filas y cabecera del dataframe IMDB\n",
        "        print(f\"\\n3. Resumen de IMDB Dataframe :\")\n",
        "        print(f\"Total Filas: {df_imdb_top_250.shape[0]}\")\n",
        "        print(\"Dataframe Cabecera:\")\n",
        "        print(df_imdb_top_250.head())\n",
        "        # Guarda el dataframe a un archivo CSV\n",
        "        df_imdb_top_250.to_csv(\"imdb_top_250.csv\", index=False)\n",
        "    else:\n",
        "        print(f\"\\n3. No se pudo obtener informacion de: {url}\")\n",
        "except Exception as e:\n",
        "    print(f\"Error: {e}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5SBa2k-puGH_",
        "outputId": "502f6079-60f0-4e44-eab6-cf1b7b47d899"
      },
      "id": "5SBa2k-puGH_",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "1. URL para Web Scrapping: https://www.imdb.com/chart/top/\n",
            "\n",
            "2. Solicitud HTTP exitosa con codigo de estado de respuesta: 200\n",
            "\n",
            "3. Resumen de IMDB Dataframe :\n",
            "Total Filas: 250\n",
            "Dataframe Cabecera:\n",
            "   rankings                    titulo  anio  duracion_minutos clasificacion  \\\n",
            "0         1  The Shawshank Redemption  1994               142             R   \n",
            "1         2             The Godfather  1972               175             R   \n",
            "2         3           The Dark Knight  2008               152         PG-13   \n",
            "3         4     The Godfather Part II  1974               202             R   \n",
            "4         5              12 Angry Men  1957                96      Approved   \n",
            "\n",
            "   calificacion  votos_millones  \n",
            "0           9.3             3.2  \n",
            "1           9.2             2.2  \n",
            "2           9.1             3.1  \n",
            "3           9.0             1.5  \n",
            "4           9.0             1.0  \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "M82r5mVespTq"
      },
      "id": "M82r5mVespTq",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "id": "3d29b9f7-cf0d-492b-a330-8e1b4f75a819",
      "metadata": {
        "id": "3d29b9f7-cf0d-492b-a330-8e1b4f75a819"
      },
      "source": [
        "### Parte 5: APIs (4 puntos)\n",
        "\n",
        "En esta actividad se utilizará la API del BCRP para construir un DataFrame a partir de diversas series de datos.\n",
        "En ese sentido, debe completar la función `obtener_datos_bcrp`. Como apoyo, se proporciona un ejemplo del comportamiento esperado del resultado, así como la URL base que se emplea para realizar las consultas en cada caso.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def obtener_datos_bcrp(codigos_series, fecha_inicio=None, fecha_fin=None):\n",
        "    \"\"\"\n",
        "    Obtiene datos de la API del BCRP y los convierte a DataFrame\n",
        "\n",
        "    Parámetros:\n",
        "    -----------\n",
        "    codigos_series : str o list\n",
        "        Código(s) de las series a consultar (ej: 'PN01207PM' o ['PD04637PD', 'PD04638PD'])\n",
        "    fecha_inicio : str, opcional\n",
        "        Fecha de inicio en formato 'YYYY-M' para series mensuales o 'YYYY-M-D' para series diarias\n",
        "    fecha_fin : str, opcional\n",
        "        Fecha de fin en formato 'YYYY-M' para series mensuales o 'YYYY-M-D' para series diarias\n",
        "\n",
        "    Retorna:\n",
        "    --------\n",
        "    DataFrame con los datos obtenidos y los códigos de series como nombres de columnas\n",
        "    \"\"\"\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "UvMzsDFHtsqL"
      },
      "id": "UvMzsDFHtsqL",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cdbc0a02",
      "metadata": {
        "id": "cdbc0a02",
        "outputId": "4d17cd6f-aa79-4509-a16d-f0aa18b5cded"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Consultando: https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PN01207PM/json\n",
            "    periodo  PN01207PM\n",
            "0  Dic.2023   3.733942\n",
            "1  Ene.2024   3.740743\n",
            "2  Feb.2024   3.827938\n",
            "3  Mar.2024   3.708989\n",
            "4  Abr.2024   3.715032\n",
            "Consultando: https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PD04637PD-PD04638PD/json/2024-11-1/2024-12-31\n",
            "     periodo  PD04637PD  PD04638PD\n",
            "0  04.Nov.24   3.766333   3.768667\n",
            "1  05.Nov.24   3.772500   3.774833\n",
            "2  06.Nov.24   3.781500   3.784833\n",
            "3  07.Nov.24   3.752167   3.755833\n",
            "4  08.Nov.24   3.767167   3.769500\n"
          ]
        }
      ],
      "source": [
        "## NO MODIFICAR - EJEMPLOS DE USO ##\n",
        "## Debe funcionar correctamente una vez que la función esté implementada ##\n",
        "\n",
        "## Ejemplo 1: Consulta simple - Una serie\n",
        "# Tipo de cambio interbancario promedio mensual\n",
        "df1 = obtener_datos_bcrp('PN01207PM')\n",
        "print(\"Consultando: https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PN01207PM/json\")\n",
        "print(df1.head())\n",
        "\n",
        "## Ejemplo 2: Múltiples series con rango de fechas\n",
        "#Tipo de cambio interbancario diario - Compra y Venta\n",
        "series = ['PD04637PD', 'PD04638PD']\n",
        "df2 = obtener_datos_bcrp(series, fecha_inicio='2024-11-1', fecha_fin='2024-12-31')\n",
        "print(\"Consultando: https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PD04637PD-PD04638PD/json/2024-11-1/2024-12-31\")\n",
        "print(df2.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "***SOLUCION***"
      ],
      "metadata": {
        "id": "2DwuAa4PbQPx"
      },
      "id": "2DwuAa4PbQPx"
    },
    {
      "cell_type": "code",
      "source": [
        "# Importar módulos requeridos\n",
        "import requests\n",
        "import pandas as pd\n",
        "\n",
        "def obtener_datos_bcrp(codigos_series, fecha_inicio=None, fecha_fin=None):\n",
        "    \"\"\"\n",
        "    Obtiene datos del URL del BCRP\n",
        "    Argumentos:\n",
        "      codigos_serie: codigo(s) de serie a consultar. Si son varios se ingresa como lista\n",
        "      fecha_inicio: fecha de inicio de la consulta\n",
        "      fecha_fin: fecha de fin de la consulta\n",
        "    Retorna:\n",
        "      df: DataFrame con los datos obtenidos\n",
        "    \"\"\"\n",
        "\n",
        "    # 1. Validar datos de entrada\n",
        "    if not isinstance(codigos_series, list):\n",
        "        codigos_series = [codigos_series]\n",
        "\n",
        "    # 2. Definir URL a consultar enbase a codigos de serie y fechas de consulta\n",
        "    BCRP_URL = \"https://estadisticas.bcrp.gob.pe/estadisticas/series/api/\"\n",
        "    if len(codigos_series) == 1:\n",
        "        url = f\"{BCRP_URL}{codigos_series[0]}/json\"\n",
        "    else:\n",
        "        url = f\"{BCRP_URL}{'-'.join(codigos_series)}/json\"\n",
        "    if fecha_inicio and fecha_fin:\n",
        "        url += f\"/{fecha_inicio}/{fecha_fin}\"\n",
        "\n",
        "    # 3. Realizar solicitud GET a la API de BCRP con el URL definido\n",
        "    try:\n",
        "        print(f'\"Consultando: {url}\"')\n",
        "        response = requests.get(url)\n",
        "        # Si respuesta correcta de URL\n",
        "        if response.status_code == 200:\n",
        "            # Guarda respuesta en formato json para mejor lectura\n",
        "            data = response.json()\n",
        "            data_list = []\n",
        "            # Lee valores segun periodo\n",
        "            for period in data['periods']:\n",
        "                data_dict = {}\n",
        "                data_dict['periodo'] = period['name']\n",
        "                for serie, value in zip(codigos_series,period['values']):\n",
        "                    # Leer el valor y redondearlo a 6 digitos decimales\n",
        "                    data_dict[serie] = round(float(value),6)\n",
        "                data_list.append(data_dict)\n",
        "            # Convierte la lista de diccionarios a Dataframe\n",
        "            df = pd.DataFrame(data_list)\n",
        "            return df\n",
        "        # Si respuesta no correcta de URL\n",
        "        else:\n",
        "            print(f\"Error en la solicitud: {response.status_code}\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error en la solicitud: {e}\")\n"
      ],
      "metadata": {
        "id": "uP530gdpasBz"
      },
      "id": "uP530gdpasBz",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df1 = obtener_datos_bcrp('PN01207PM')\n",
        "print(df1.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fKA-Qz-lbAny",
        "outputId": "51a7c397-bb1d-46d9-d87b-ac14c123b719"
      },
      "id": "fKA-Qz-lbAny",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Consultando: https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PN01207PM/json\"\n",
            "    periodo  PN01207PM\n",
            "0  Ene.2021   3.624950\n",
            "1  Feb.2021   3.645690\n",
            "2  Mar.2021   3.709178\n",
            "3  Abr.2021   3.699525\n",
            "4  May.2021   3.774757\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "series = ['PD04637PD', 'PD04638PD']\n",
        "df2 = obtener_datos_bcrp(series, fecha_inicio='2024-11-1', fecha_fin='2024-12-31')\n",
        "print(df2.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_vmYnzvubBKp",
        "outputId": "f4bf9ade-3e0d-439d-fb91-134d0802bd63"
      },
      "id": "_vmYnzvubBKp",
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\"Consultando: https://estadisticas.bcrp.gob.pe/estadisticas/series/api/PD04637PD-PD04638PD/json/2024-11-1/2024-12-31\"\n",
            "     periodo  PD04637PD  PD04638PD\n",
            "0  04.Nov.24   3.766333   3.768667\n",
            "1  05.Nov.24   3.772500   3.774833\n",
            "2  06.Nov.24   3.781500   3.784833\n",
            "3  07.Nov.24   3.752167   3.755833\n",
            "4  08.Nov.24   3.767167   3.769500\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.11"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}